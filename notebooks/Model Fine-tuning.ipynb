{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T20:42:16.596687Z","iopub.status.busy":"2024-03-18T20:42:16.595925Z","iopub.status.idle":"2024-03-18T20:42:25.406190Z","shell.execute_reply":"2024-03-18T20:42:25.405207Z","shell.execute_reply.started":"2024-03-18T20:42:16.596657Z"},"id":"B-EfdyMSAB9v","trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n","from transformers import BertTokenizer, BertModel, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from datasets import load_dataset\n","from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-03-18T20:42:30.430508Z","iopub.status.busy":"2024-03-18T20:42:30.430179Z","iopub.status.idle":"2024-03-18T20:42:46.637104Z","shell.execute_reply":"2024-03-18T20:42:46.636064Z","shell.execute_reply.started":"2024-03-18T20:42:30.430482Z"},"id":"k4gpjqhQAAw7","outputId":"6c57f4f4-d67f-427e-c112-fdf735197280","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5001a77668543bb95c8026ddcb2e2cf","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88f6a2ac09cf46f3b68e81a98ae03269","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c966d0aaed747639fb1062e7f81d0f4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"981e92829f914b4389b55c3f0e942e22","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6119bdebdd3e4538945c5b08807bc987","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b9185ca4b864b84b63bbc21da6c5f84","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eee7d49c7edd4396b44071a0975a8fbf","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cdf8a07f7494aa687b132e2d5836b05","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"236381d945b74c6595eb3d7591fa4c1d","version_major":2,"version_minor":0},"text/plain":["Downloading data: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cc22ae30bf64281b61f3b5c6d66bb7c","version_major":2,"version_minor":0},"text/plain":["Downloading data: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef23c0dc268b44368fc112fc345a8d94","version_major":2,"version_minor":0},"text/plain":["Downloading data: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8efd03650aeb4f8381b7df7d75f56fe9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/796144094.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_inputs = torch.tensor(train_inputs)\n","/tmp/ipykernel_34/796144094.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_inputs = torch.tensor(validation_inputs)\n","/tmp/ipykernel_34/796144094.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_labels = torch.tensor(train_labels)\n","/tmp/ipykernel_34/796144094.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_labels = torch.tensor(validation_labels)\n","/tmp/ipykernel_34/796144094.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_masks = torch.tensor(train_masks)\n","/tmp/ipykernel_34/796144094.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_masks = torch.tensor(validation_masks)\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy before fine-tuning: 0.6467\n"]}],"source":["# Define a new model which adds an additional layer on top of BertForSequenceClassification\n","class BertForSequenceClassificationCustom(nn.Module):\n","    def __init__(self, num_labels=2):\n","        super(BertForSequenceClassificationCustom, self).__init__()\n","        self.num_labels = num_labels\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(0.1)\n","        # Adding an additional custom layer after BERT output\n","        self.classifier = nn.Sequential(\n","            nn.Linear(768, 512),  # 768 is the size of BERT's hidden representation, adjust if using a different model\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_labels)\n","        )\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        pooled_output = outputs[1]  # We are interested in BERT's pooled output\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Function to calculate accuracy\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","# Evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","            logits = model(b_input_ids, attention_mask=b_input_mask)\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","    return total_eval_accuracy / len(dataloader)\n","\n","# Function for prediction on example sentences\n","def predict_on_example(model, tokenizer, sentence1, sentence2, device):\n","    model.eval()  # Put the model in evaluation mode\n","    inputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\", max_length=128, truncation=True, padding='max_length')\n","    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the correct device\n","    with torch.no_grad():\n","        logits = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n","    probabilities = torch.softmax(logits, dim=1)\n","    prediction = torch.argmax(probabilities, dim=1)\n","    return probabilities, prediction.item()\n","\n","# Check if a GPU is available and set the device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load tokenizer and initialize custom model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassificationCustom(num_labels=2)\n","model.to(device)  # Move model to the device\n","\n","# Load and preprocess the MRPC dataset\n","dataset = load_dataset('glue', 'mrpc')\n","texts = [(tokenizer(example['sentence1'], example['sentence2'], truncation=True, padding='max_length', max_length=128), example['label']) for example in dataset['train']]\n","input_ids = torch.tensor([t[0]['input_ids'] for t in texts])\n","attention_masks = torch.tensor([t[0]['attention_mask'] for t in texts])\n","labels = torch.tensor([t[1] for t in texts])\n","\n","# Split the dataset into training and validation sets\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n","\n","# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=32)\n","\n","# Evaluate the model before fine-tuning\n","pre_fine_tune_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Accuracy before fine-tuning: {pre_fine_tune_accuracy:.4f}')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["7e980e1458064aeabfb2be86b1d53cd6","6fe4cf7868f142f1ba9509c6c0b054e8","2d9a4ebd498e4fcaaca2b7a347fade41","25c2614d637445e08cadc3214cf23dd8","759ed936fb894544aa88ee0d423d3a88","44a25660b8b64c968f5a0486bb688eed","9a8ccdcbf7be4adc8c24e51b78b86635","f769663f2b834449ac1abd41514c87b5","ff82937e385645b5b2a5a8e1555fd3fb","6ad6b3b3ea1a41f88eb6bc06eac1e334","46d95ab3c5144c24821972be2f2b2831"]},"execution":{"iopub.execute_input":"2024-03-18T20:42:55.666470Z","iopub.status.busy":"2024-03-18T20:42:55.666118Z","iopub.status.idle":"2024-03-18T20:46:00.800152Z","shell.execute_reply":"2024-03-18T20:46:00.799137Z","shell.execute_reply.started":"2024-03-18T20:42:55.666442Z"},"id":"HDGJGTVgZw37","outputId":"691992cb-0b59-443e-e2e1-8d03ab36af12","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction before fine-tuning: 1, Probabilities: tensor([[0.4777, 0.5223]], device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a8913da6ab44880bac186f57c4b1971","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Prediction after fine-tuning: 0, Probabilities: tensor([[0.9665, 0.0335]], device='cuda:0')\n","Accuracy after fine-tuning: 0.8193\n"]}],"source":["# Setup the optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","epochs = 5  # Increase number of epochs for more fine-tuning steps\n","\n","# Total number of training steps is the number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,  # Default value\n","    num_training_steps=total_steps\n",")\n","\n","# Example sentences for prediction before fine-tuning\n","sentence1 = \"The company reported better than expected results.\"\n","sentence2 = \"The firm's results exceeded forecasts.\"\n","probabilities, prediction = predict_on_example(model, tokenizer, sentence1, sentence2, device)\n","print(f'Prediction before fine-tuning: {prediction}, Probabilities: {probabilities}')\n","\n","# Fine-tune the model\n","model.train()\n","for epoch in tqdm(range(epochs)):\n","    for batch in train_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        model.zero_grad()\n","        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","        loss = nn.CrossEntropyLoss()(logits, b_labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","# Example sentences for prediction after fine-tuning\n","probabilities, prediction = predict_on_example(model, tokenizer, sentence1, sentence2, device)\n","print(f'Prediction after fine-tuning: {prediction}, Probabilities: {probabilities}')\n","\n","# Evaluate the model after fine-tuning\n","post_fine_tune_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Accuracy after fine-tuning: {post_fine_tune_accuracy:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["## Fine tuning by unfreezeing 3 layers"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:39:17.561756Z","iopub.status.busy":"2024-03-18T21:39:17.560761Z","iopub.status.idle":"2024-03-18T21:39:17.572913Z","shell.execute_reply":"2024-03-18T21:39:17.571808Z","shell.execute_reply.started":"2024-03-18T21:39:17.561712Z"},"trusted":true},"outputs":[],"source":["def unfreeze_and_train(model, train_dataloader, validation_dataloader, device, epochs=5):\n","    # First, freeze all parameters\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    # Unfreeze the last three layers\n","    for layer in [model.bert.encoder.layer[-1], model.bert.encoder.layer[-2], model.bert.encoder.layer[-3]]:\n","        for param in layer.parameters():\n","            param.requires_grad = True\n","\n","    # Setup the optimizer for the unfrozen parameters\n","    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)\n","\n","    # Recalculate the number of steps and prepare the scheduler again since we changed the training parameters\n","    total_steps = len(train_dataloader) * epochs\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","    # Fine-tune the model\n","    model.train()\n","    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            \n","            model.zero_grad()\n","\n","            logits = model(b_input_ids, attention_mask=b_input_mask)\n","            loss = nn.CrossEntropyLoss()(logits, b_labels)\n","            \n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        print(f\"Average training loss: {avg_train_loss:.4f}\")\n","\n","    print(\"Finished fine-tuning.\")\n","\n","    # Evaluate the model after fine-tuning\n","    post_fine_tune_accuracy = evaluate_model(model, validation_dataloader, device)\n","    print(f'Accuracy after fine-tuning: {post_fine_tune_accuracy:.4f}')\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:39:20.545615Z","iopub.status.busy":"2024-03-18T21:39:20.545241Z","iopub.status.idle":"2024-03-18T21:40:50.338361Z","shell.execute_reply":"2024-03-18T21:40:50.337445Z","shell.execute_reply.started":"2024-03-18T21:39:20.545584Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4b2ce0bd4ce4b858467b4cae172ffdf","version_major":2,"version_minor":0},"text/plain":["Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average training loss: 0.0926\n","Average training loss: 0.0790\n","Average training loss: 0.0694\n","Average training loss: 0.0662\n","Average training loss: 0.0608\n","Finished fine-tuning.\n","Accuracy after fine-tuning: 0.8141\n"]}],"source":["# Assuming you have defined 'model', 'train_dataloader', 'validation_dataloader', and 'device' as per your notebook setup\n","epochs = 5  # You can adjust the number of epochs based on your requirement\n","\n","# Call the function to unfreeze the last three layers and fine-tune the model\n","unfreeze_and_train(model, train_dataloader, validation_dataloader, device, epochs=epochs)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4607107,"sourceId":7855056,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"25c2614d637445e08cadc3214cf23dd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ad6b3b3ea1a41f88eb6bc06eac1e334","placeholder":"​","style":"IPY_MODEL_46d95ab3c5144c24821972be2f2b2831","value":" 5/5 [05:34&lt;00:00, 66.85s/it]"}},"2d9a4ebd498e4fcaaca2b7a347fade41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f769663f2b834449ac1abd41514c87b5","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff82937e385645b5b2a5a8e1555fd3fb","value":5}},"44a25660b8b64c968f5a0486bb688eed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46d95ab3c5144c24821972be2f2b2831":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ad6b3b3ea1a41f88eb6bc06eac1e334":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fe4cf7868f142f1ba9509c6c0b054e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44a25660b8b64c968f5a0486bb688eed","placeholder":"​","style":"IPY_MODEL_9a8ccdcbf7be4adc8c24e51b78b86635","value":"100%"}},"759ed936fb894544aa88ee0d423d3a88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e980e1458064aeabfb2be86b1d53cd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fe4cf7868f142f1ba9509c6c0b054e8","IPY_MODEL_2d9a4ebd498e4fcaaca2b7a347fade41","IPY_MODEL_25c2614d637445e08cadc3214cf23dd8"],"layout":"IPY_MODEL_759ed936fb894544aa88ee0d423d3a88"}},"9a8ccdcbf7be4adc8c24e51b78b86635":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f769663f2b834449ac1abd41514c87b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff82937e385645b5b2a5a8e1555fd3fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
